# -*- coding: utf-8 -*-
"""lstm_enc_dec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wCDyoomqNCYspU22HTwlOucFh9uW9nx
"""

!pip install tensorflow

!wget http://www.manythings.org/anki/spa-eng.zip

!mkdir -p data && unzip spa-eng.zip -d data

import string
import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy


# Path to translation file
path_to_data = 'data/spa.txt'

# Read file
translation_file = open(path_to_data,"r", encoding='utf-8')
raw_data = translation_file.read()
translation_file.close()

# Parse data
raw_data = raw_data.split('\n')
pairs = [sentence.split('\t') for sentence in  raw_data]
pairs = pairs[1000:20000]

def clean_sentence(sentence):
    # Lower case the sentence
    lower_case_sent = sentence.lower()
    # Strip punctuation
    string_punctuation = string.punctuation + "¡" + '¿'
    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))

    return clean_sentence

def tokenize(sentences):
    # Create tokenizer
    text_tokenizer = Tokenizer()
    # Fit texts
    text_tokenizer.fit_on_texts(sentences)
    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer

# Clean sentences
english_sentences = [clean_sentence(pair[0]) for pair in pairs]
spanish_sentences = [clean_sentence(pair[1]) for pair in pairs]

# Tokenize words
spa_text_tokenized, spa_text_tokenizer = tokenize(spanish_sentences)
eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)

print('Maximum length spanish sentence: {}'.format(len(max(spa_text_tokenized,key=len))))
print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))


# Check language length
spanish_vocab = len(spa_text_tokenizer.word_index) + 1
english_vocab = len(eng_text_tokenizer.word_index) + 1
print("Spanish vocabulary is of {} unique words".format(spanish_vocab))
print("English vocabulary is of {} unique words".format(english_vocab))

max_spanish_len = int(len(max(spa_text_tokenized,key=len)))
max_english_len = int(len(max(eng_text_tokenized,key=len)))

spa_pad_sentence = pad_sequences(spa_text_tokenized, max_spanish_len, padding = "post")
eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding = "post")

# Reshape data
spa_pad_sentence = spa_pad_sentence.reshape(*spa_pad_sentence.shape, 1)
eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)

input_sequence = Input(shape=(max_spanish_len,))
embedding = Embedding(input_dim=spanish_vocab, output_dim=128,)(input_sequence)
encoder = LSTM(64, return_sequences=False)(embedding)
r_vec = RepeatVector(max_english_len)(encoder)
decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)
logits = TimeDistributed(Dense(english_vocab))(decoder)

enc_dec_model = Model(input_sequence, Activation('softmax')(logits))
enc_dec_model.compile(loss=sparse_categorical_crossentropy,
              optimizer=Adam(1e-3),
              metrics=['accuracy'])
enc_dec_model.summary()

model_results = enc_dec_model.fit(spa_pad_sentence, eng_pad_sentence, batch_size=30, epochs=100)

def logits_to_sentence(logits, tokenizer):

    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}
    index_to_words[0] = '<empty>'

    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

index = 14
print("The english sentence is: {}".format(english_sentences[index]))
print("The spanish sentence is: {}".format(spanish_sentences[index]))
print('The predicted sentence is :')
print(logits_to_sentence(enc_dec_model.predict(spa_pad_sentence[index:index+1])[0], eng_text_tokenizer))

import numpy as np
import random

def logits_to_sentence(logits, tokenizer):
    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}
    index_to_words[0] = '<empty>'
    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

def calculate_accuracy(model, tokenizer, spanish_sentences, english_sentences, pad_sentences, n=100, max_index=3000):
    # Randomly select 100 indices within 3000 index
    random_indices = random.sample(range(max_index), n)
    correct_predictions = 0

    # Lists to store predictions and ground truths
    predictions_list = []
    groundtruths_list = []
    correctness_list = []

    for index in random_indices:
        # Get the Spanish sentence, predict, and convert logits to sentence
        predicted_sentence = logits_to_sentence(model.predict(pad_sentences[index:index+1])[0], tokenizer)

        # Get the ground truth English sentence
        ground_truth_sentence = english_sentences[index]

        # Store predictions and ground truths
        predictions_list.append(predicted_sentence.replace('<empty>', '').strip())
        groundtruths_list.append(ground_truth_sentence.strip())

        # Check if the predicted sentence matches the ground truth (ignoring <empty> tokens)
        if predictions_list[-1] == groundtruths_list[-1]:
            correctness_list.append("Correct")
            correct_predictions += 1
        else:
            correctness_list.append("Wrong")

    # Calculate accuracy
    accuracy = correct_predictions / n * 100

    # Output predictions, ground truths, and correctness
    for i, (predicted, ground_truth, correctness) in enumerate(zip(predictions_list, groundtruths_list, correctness_list)):
        print(f"Index {i+1}:")
        print(f"  Predicted: {predicted}")
        print(f"  Ground Truth: {ground_truth}")
        print(f"  Status: {correctness}")
        print()

    # Return the accuracy
    return accuracy

# Calculate accuracy for 100 random sentences within the first 3000 indices
accuracy = calculate_accuracy(enc_dec_model, eng_text_tokenizer, spanish_sentences, english_sentences, spa_pad_sentence, n=100, max_index=3000)
print(f'Accuracy: {accuracy:.2f}%')

import matplotlib.pyplot as plt
import numpy as np
import random

def logits_to_sentence(logits, tokenizer):
    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}
    index_to_words[0] = '<empty>'
    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

def calculate_accuracy_and_collect_data(model, tokenizer, spanish_sentences, english_sentences, pad_sentences, n=100, max_index=3000):
    # Randomly select 100 indices within 3000 index
    random_indices = random.sample(range(max_index), n)
    correct_predictions = 0

    # Lists to store predictions, ground truths, correctness, and sentence lengths
    predictions_list = []
    groundtruths_list = []
    correctness_list = []
    predicted_lengths = []
    ground_truth_lengths = []

    for index in random_indices:
        # Get the Spanish sentence, predict, and convert logits to sentence
        predicted_sentence = logits_to_sentence(model.predict(pad_sentences[index:index+1])[0], tokenizer)

        # Get the ground truth English sentence
        ground_truth_sentence = english_sentences[index]

        # Store predictions, ground truths, and lengths
        predictions_list.append(predicted_sentence.replace('<empty>', '').strip())
        groundtruths_list.append(ground_truth_sentence.strip())
        predicted_lengths.append(len(predictions_list[-1].split()))
        ground_truth_lengths.append(len(groundtruths_list[-1].split()))

        # Check if the predicted sentence matches the ground truth (ignoring <empty> tokens)
        if predictions_list[-1] == groundtruths_list[-1]:
            correctness_list.append("Correct")
            correct_predictions += 1
        else:
            correctness_list.append("Wrong")

    # Calculate accuracy
    accuracy = correct_predictions / n * 100

    # Return data
    return accuracy, correctness_list, predicted_lengths, ground_truth_lengths

# Calculate accuracy and collect data for plotting
accuracy, correctness_list, predicted_lengths, ground_truth_lengths = calculate_accuracy_and_collect_data(
    enc_dec_model, eng_text_tokenizer, spanish_sentences, english_sentences, spa_pad_sentence, n=100, max_index=3000)

# Plot the correctness breakdown (Pie chart)
correct_count = correctness_list.count("Correct")
wrong_count = correctness_list.count("Wrong")

plt.figure(figsize=(10, 5))

# Pie chart for correctness
plt.subplot(1, 2, 1)
plt.pie([correct_count, wrong_count], labels=['Correct', 'Wrong'], autopct='%1.1f%%', colors=['#66b3ff', '#ff9999'])
plt.title('Prediction Accuracy Breakdown')

# Bar chart for sentence lengths
plt.subplot(1, 2, 2)
plt.bar(range(1, 101), predicted_lengths, alpha=0.6, label='Predicted Sentence Length')
plt.bar(range(1, 101), ground_truth_lengths, alpha=0.6, label='Ground Truth Sentence Length')
plt.xlabel('Sentence Index')
plt.ylabel('Sentence Length (words)')
plt.title('Sentence Length Comparison')
plt.legend()

# Show both plots
plt.tight_layout()
plt.show()

# Print the accuracy
print(f'Accuracy: {accuracy:.2f}%')